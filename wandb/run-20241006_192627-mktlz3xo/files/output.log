You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards: 100%|██████████| 8/8 [00:00<00:00,  9.49it/s]
capped samples at max_samples=999999999
